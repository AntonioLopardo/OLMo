W1212 12:19:27.932000 47095 torch/distributed/run.py:803] 
W1212 12:19:27.932000 47095 torch/distributed/run.py:803] *****************************************
W1212 12:19:27.932000 47095 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1212 12:19:27.932000 47095 torch/distributed/run.py:803] *****************************************
2025-12-12 12:19:47.999	quick-rain-shines-fin-03:0	train:417	INFO	CLI environment prepared
2025-12-12 12:19:49.168	quick-rain-shines-fin-03:0	train:97	INFO	Configuration:
2025-12-12 12:19:49.168	quick-rain-shines-fin-03:0	train:98	INFO	TrainConfig(run_name='wandb-test-run', seed=6198, epoch=None, dry_run=False, model=ModelConfig(d_model=2048, n_heads=16, n_kv_heads=None, clip_qkv=8.0, n_layers=16, mlp_ratio=8, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', block_group_size=1, alibi=False, alibi_bias_max=8.0, rope=True, rope_full_precision=True, rope_theta=10000, flash_attention=True, attention_dropout=0.0, multi_query_attention=False, attention_layer_norm=False, residual_dropout=0.0, embedding_dropout=0.0, embedding_layer_norm=False, layer_norm_type='default', layer_norm_with_affine=False, layer_norm_eps=1e-05, attention_layer_norm_with_affine=False, max_sequence_length=4096, include_bias=False, bias_for_layer_norm=False, scale_logits=False, vocab_size=50280, embedding_size=50304, weight_tying=False, eos_token_id=50279, pad_token_id=1, init_device='cuda', init_fn='mitchell', init_std=0.02, init_cutoff_factor=None, precision='amp_bf16', scale_emb_init=False, emb_init_std=None, norm_after=False), optimizer=OptimizerConfig(name='adamw', learning_rate=0.0003, weight_decay=0.1, betas=(0.9, 0.95), eps=1e-05, no_decay_norm_and_bias=None, selective_updates=False, decay_norm_and_bias=True, decay_embeddings=True, metrics_log_interval=5, record_update_metrics=False), scheduler=SchedulerConfig(name='cosine_with_warmup', units='steps', t_warmup=10, t_max=None, alpha_f=0.1, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=None), data=DataConfig(paths=['/home/vec_norm/OLMo/data/dolma_v1_7/dolma_v1_7_30B.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, generate_doc_lengths=False, num_workers=8, drop_last=True, pin_memory=True, prefetch_factor=2, persistent_workers=True, timeout=0, seed=None, instance_filter=None, custom_dataset=None), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=1000, tokenizer=TokenizerConfig(identifier='tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json', truncate_direction='right'), save_folder='/home/vec_norm/OLMo/checkpoints/wandb-test', remote_save_folder=None, canceled_check_interval=50, save_interval=1000, save_interval_unsharded=None, save_interval_ephemeral=None, save_num_checkpoints_to_keep=1, save_num_unsharded_checkpoints_to_keep=0, save_overwrite=True, force_save_unsharded=False, no_pre_train_checkpoint=True, load_path=None, load_path_sharded_checkpointer=None, try_load_latest_save=False, reset_optimizer_state=False, reset_trainer_state=False, sharded_checkpointer='torch_legacy', new_style_checkpoints=None, max_duration=20, global_train_batch_size=512, device_train_batch_size=64, device_train_microbatch_size=8, device_eval_batch_size=16, eval_subset_num_batches=-1, eval_on_load=False, device_train_grad_accum=8, max_grad_norm=1.0, max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='VecNorm', entity='antoniolopardo', group='wandb-tests', name='wandb-test-run', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=1), speed_monitor=SpeedMonitorConfig(window_size=5, gpu_flops_available=None), console_log_interval=1, gen1_gc_interval=1, compile=None, distributed_strategy='ddp', fsdp=FSDPConfig(use_orig_params=True, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, wrapping_strategy=None, precision='pure', hybrid_sharding_num_model_replicas=None), ddp=DDPConfig(grad_sync_mode='batch', find_unused_params=False), single=SingleGPUConfig(device='auto'), softmax_auxiliary_loss=False, auxiliary_loss_multiplier=0.0001, time_limit=None, extra_steps_after_cancel=10, early_stopping_factor=None, save_data_indices=True, python_profiling=False, torch_profiling=False, stop_at=None, stop_after=None, activation_checkpointing=None, fused_loss=None, hf_datasets_cache_dir=None, module_outputs_save_steps=None)
2025-12-12 12:19:49.169	quick-rain-shines-fin-03:0	train:105	INFO	Saving config to /home/vec_norm/OLMo/checkpoints/wandb-test/config.yaml
wandb: Currently logged in as: antoniolopardo to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run ujz0ph6t
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/vec_norm/OLMo/checkpoints/wandb-test/wandb/wandb/run-20251212_121949-ujz0ph6t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandb-test-run
wandb: ‚≠êÔ∏è View project at https://wandb.ai/antoniolopardo/VecNorm
wandb: üöÄ View run at https://wandb.ai/antoniolopardo/VecNorm/runs/ujz0ph6t
2025-12-12 12:19:52.921	quick-rain-shines-fin-03:0	olmo.data.iterable_dataset:79	INFO	Saving global data order indices...
2025-12-12 12:19:53.252	quick-rain-shines-fin-03:0	olmo.data.iterable_dataset:88	INFO	Global data order indices saved to '/home/vec_norm/OLMo/checkpoints/wandb-test/train_data/global_indices.npy'
2025-12-12 12:19:53.387	quick-rain-shines-fin-03:0	train:139	INFO	Building model...
2025-12-12 12:19:53.599	quick-rain-shines-fin-03:0	olmo.model:1174	INFO	Initializing model parameters...
2025-12-12 12:19:53.654	quick-rain-shines-fin-03:0	train:141	INFO	Total number of parameters: 1,279,787,008
2025-12-12 12:19:53.655	quick-rain-shines-fin-03:0	train:142	INFO	Number of non-embedding parameters: 1,176,764,416
2025-12-12 12:19:53.677	quick-rain-shines-fin-03:0	train:143	INFO	Peak GPU Memory (MB) before ddp: 5125
2025-12-12 12:19:53.677	quick-rain-shines-fin-03:0	train:155	INFO	Wrapping model with DDP...
2025-12-12 12:19:53.698	quick-rain-shines-fin-03:0	olmo.model:1174	INFO	Initializing model parameters...
2025-12-12 12:19:53.707	quick-rain-shines-fin-03:0	train:232	INFO	Peak GPU Memory (MB) after ddp: 10244
2025-12-12 12:19:53.707	quick-rain-shines-fin-03:0	train:233	INFO	Model:
2025-12-12 12:19:53.708	quick-rain-shines-fin-03:0	train:234	INFO	DistributedDataParallel(
  (module): OLMo(
    (transformer): ModuleDict(
      (wte): Embedding(50304, 2048)
      (emb_drop): Dropout(p=0.0, inplace=False)
      (ln_f): LayerNorm()
      (blocks): ModuleList(
        (0-15): 16 x OLMoSequentialBlock(
          (dropout): Dropout(p=0.0, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)
          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)
          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
        )
      )
      (ff_out): Linear(in_features=2048, out_features=50304, bias=False)
    )
  )
)
2025-12-12 12:19:53.710	quick-rain-shines-fin-03:0	olmo.optim:944	INFO	Constructing optimizer with 1 param groups
2025-12-12 12:19:53.711	quick-rain-shines-fin-03:0	train:375	INFO	Starting training...
2025-12-12 12:19:53.712	quick-rain-shines-fin-03:0	olmo.train:979	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=10,244
2025-12-12 12:20:10.218	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=1/20,epoch=0]
    optim/total_grad_norm=11.53
    train/CrossEntropyLoss=11.24
    train/Perplexity=76,058
    throughput/total_tokens=2,097,152
    throughput/total_training_Gflops=19,049,038
    throughput/total_training_log_Gflops=16.76
    System/Peak GPU Memory (MB)=101,234
2025-12-12 12:20:14.034	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=2/20,epoch=0]
    optim/total_grad_norm=103.5
    train/CrossEntropyLoss=12.07
    train/Perplexity=174,515
    throughput/total_tokens=4,194,304
    throughput/total_training_Gflops=38,098,077
    throughput/total_training_log_Gflops=17.46
    throughput/device/tokens_per_second=68,764
    throughput/device/batches_per_second=0.2623
    System/Peak GPU Memory (MB)=111,477
2025-12-12 12:20:17.855	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=3/20,epoch=0]
    optim/total_grad_norm=12.91
    train/CrossEntropyLoss=10.91
    train/Perplexity=54,723
    throughput/total_tokens=6,291,456
    throughput/total_training_Gflops=57,147,116
    throughput/total_training_log_Gflops=17.86
    throughput/device/tokens_per_second=68,682
    throughput/device/batches_per_second=0.2620
2025-12-12 12:20:21.669	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=4/20,epoch=0]
    optim/total_grad_norm=22.60
    train/CrossEntropyLoss=11.55
    train/Perplexity=103,867
    throughput/total_tokens=8,388,608
    throughput/total_training_Gflops=76,196,155
    throughput/total_training_log_Gflops=18.15
    throughput/device/tokens_per_second=68,699
    throughput/device/batches_per_second=0.2621
2025-12-12 12:20:25.568	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=5/20,epoch=0]
    optim/total_grad_norm=9.544
    train/CrossEntropyLoss=11.31
    train/Perplexity=81,458
    throughput/total_tokens=10,485,760
    throughput/total_training_Gflops=95,245,194
    throughput/total_training_log_Gflops=18.37
    throughput/device/tokens_per_second=68,327
    throughput/device/batches_per_second=0.2606
2025-12-12 12:20:29.383	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=6/20,epoch=0]
    optim/total_grad_norm=6.923
    train/CrossEntropyLoss=10.85
    train/Perplexity=51,623
    throughput/total_tokens=12,582,912
    throughput/total_training_Gflops=114,294,233
    throughput/total_training_log_Gflops=18.55
    throughput/device/tokens_per_second=68,401
    throughput/device/batches_per_second=0.2609
2025-12-12 12:20:33.193	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=7/20,epoch=0]
    optim/total_grad_norm=20.85
    train/CrossEntropyLoss=10.53
    train/Perplexity=37,519
    throughput/total_tokens=14,680,064
    throughput/total_training_Gflops=133,343,272
    throughput/total_training_log_Gflops=18.71
    throughput/device/tokens_per_second=68,416
    throughput/device/batches_per_second=0.2610
2025-12-12 12:20:37.000	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=8/20,epoch=0]
    optim/total_grad_norm=3.759
    train/CrossEntropyLoss=10.08
    train/Perplexity=23,849
    throughput/total_tokens=16,777,216
    throughput/total_training_Gflops=152,392,311
    throughput/total_training_log_Gflops=18.84
    throughput/device/tokens_per_second=68,468
    throughput/device/batches_per_second=0.2612
2025-12-12 12:20:40.798	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=9/20,epoch=0]
    optim/total_grad_norm=3.267
    train/CrossEntropyLoss=9.509
    train/Perplexity=13,484
    throughput/total_tokens=18,874,368
    throughput/total_training_Gflops=171,441,350
    throughput/total_training_log_Gflops=18.96
    throughput/device/tokens_per_second=68,525
    throughput/device/batches_per_second=0.2614
2025-12-12 12:20:44.660	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=10/20,epoch=0]
    optim/total_grad_norm=7.524
    train/CrossEntropyLoss=9.062
    train/Perplexity=8,622
    throughput/total_tokens=20,971,520
    throughput/total_training_Gflops=190,490,389
    throughput/total_training_log_Gflops=19.07
    throughput/device/tokens_per_second=68,677
    throughput/device/batches_per_second=0.2620
    System/Peak GPU Memory (MB)=111,477
2025-12-12 12:20:48.461	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=11/20,epoch=0]
    optim/total_grad_norm=3.599
    train/CrossEntropyLoss=8.706
    train/Perplexity=6,037
    throughput/total_tokens=23,068,672
    throughput/total_training_Gflops=209,539,428
    throughput/total_training_log_Gflops=19.16
    throughput/device/tokens_per_second=68,709
    throughput/device/batches_per_second=0.2621
2025-12-12 12:20:52.268	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=12/20,epoch=0]
    optim/total_grad_norm=4.210
    train/CrossEntropyLoss=8.551
    train/Perplexity=5,174
    throughput/total_tokens=25,165,824
    throughput/total_training_Gflops=228,588,467
    throughput/total_training_log_Gflops=19.25
    throughput/device/tokens_per_second=68,722
    throughput/device/batches_per_second=0.2622
2025-12-12 12:20:56.069	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=13/20,epoch=0]
    optim/total_grad_norm=4.914
    train/CrossEntropyLoss=8.423
    train/Perplexity=4,552
    throughput/total_tokens=27,262,976
    throughput/total_training_Gflops=247,637,506
    throughput/total_training_log_Gflops=19.33
    throughput/device/tokens_per_second=68,739
    throughput/device/batches_per_second=0.2622
2025-12-12 12:20:59.870	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=14/20,epoch=0]
    optim/total_grad_norm=2.069
    train/CrossEntropyLoss=8.249
    train/Perplexity=3,822
    throughput/total_tokens=29,360,128
    throughput/total_training_Gflops=266,686,545
    throughput/total_training_log_Gflops=19.40
    throughput/device/tokens_per_second=68,726
    throughput/device/batches_per_second=0.2622
2025-12-12 12:21:03.742	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=15/20,epoch=0]
    optim/total_grad_norm=1.978
    train/CrossEntropyLoss=8.114
    train/Perplexity=3,341
    throughput/total_tokens=31,457,280
    throughput/total_training_Gflops=285,735,584
    throughput/total_training_log_Gflops=19.47
    throughput/device/tokens_per_second=68,704
    throughput/device/batches_per_second=0.2621
2025-12-12 12:21:07.549	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=16/20,epoch=0]
    optim/total_grad_norm=1.510
    train/CrossEntropyLoss=8.037
    train/Perplexity=3,091
    throughput/total_tokens=33,554,432
    throughput/total_training_Gflops=304,784,623
    throughput/total_training_log_Gflops=19.54
    throughput/device/tokens_per_second=68,673
    throughput/device/batches_per_second=0.2620
2025-12-12 12:21:11.357	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=17/20,epoch=0]
    optim/total_grad_norm=2.085
    train/CrossEntropyLoss=8.021
    train/Perplexity=3,044
    throughput/total_tokens=35,651,584
    throughput/total_training_Gflops=323,833,662
    throughput/total_training_log_Gflops=19.60
    throughput/device/tokens_per_second=68,667
    throughput/device/batches_per_second=0.2619
2025-12-12 12:21:15.166	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=18/20,epoch=0]
    optim/total_grad_norm=2.235
    train/CrossEntropyLoss=7.980
    train/Perplexity=2,921
    throughput/total_tokens=37,748,736
    throughput/total_training_Gflops=342,882,701
    throughput/total_training_log_Gflops=19.65
    throughput/device/tokens_per_second=68,643
    throughput/device/batches_per_second=0.2619
2025-12-12 12:21:18.974	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=19/20,epoch=0]
    optim/total_grad_norm=1.810
    train/CrossEntropyLoss=7.967
    train/Perplexity=2,884
    throughput/total_tokens=39,845,888
    throughput/total_training_Gflops=361,931,740
    throughput/total_training_log_Gflops=19.71
    throughput/device/tokens_per_second=68,615
    throughput/device/batches_per_second=0.2617
2025-12-12 12:21:22.847	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=20/20,epoch=0]
    optim/total_grad_norm=1.169
    train/CrossEntropyLoss=7.960
    train/Perplexity=2,863
    throughput/total_tokens=41,943,040
    throughput/total_training_Gflops=380,980,779
    throughput/total_training_log_Gflops=19.76
    throughput/device/tokens_per_second=68,641
    throughput/device/batches_per_second=0.2618
    System/Peak GPU Memory (MB)=111,477
2025-12-12 12:21:26.648	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=21/20,epoch=0]
    optim/total_grad_norm=0.8484
    train/CrossEntropyLoss=7.946
    train/Perplexity=2,824
    throughput/total_tokens=44,040,192
    throughput/total_training_Gflops=400,029,817
    throughput/total_training_log_Gflops=19.81
    throughput/device/tokens_per_second=68,633
    throughput/device/batches_per_second=0.2618
2025-12-12 12:21:30.466	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=22/20,epoch=0]
    optim/total_grad_norm=0.8361
    train/CrossEntropyLoss=7.900
    train/Perplexity=2,697
    throughput/total_tokens=46,137,344
    throughput/total_training_Gflops=419,078,856
    throughput/total_training_log_Gflops=19.85
    throughput/device/tokens_per_second=68,596
    throughput/device/batches_per_second=0.2617
2025-12-12 12:21:34.280	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=23/20,epoch=0]
    optim/total_grad_norm=0.9520
    train/CrossEntropyLoss=7.890
    train/Perplexity=2,671
    throughput/total_tokens=48,234,496
    throughput/total_training_Gflops=438,127,895
    throughput/total_training_log_Gflops=19.90
    throughput/device/tokens_per_second=68,576
    throughput/device/batches_per_second=0.2616
2025-12-12 12:21:38.091	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=24/20,epoch=0]
    optim/total_grad_norm=0.9583
    train/CrossEntropyLoss=7.884
    train/Perplexity=2,653
    throughput/total_tokens=50,331,648
    throughput/total_training_Gflops=457,176,934
    throughput/total_training_log_Gflops=19.94
    throughput/device/tokens_per_second=68,570
    throughput/device/batches_per_second=0.2616
2025-12-12 12:21:41.961	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=25/20,epoch=0]
    optim/total_grad_norm=0.8712
    train/CrossEntropyLoss=7.906
    train/Perplexity=2,713
    throughput/total_tokens=52,428,800
    throughput/total_training_Gflops=476,225,973
    throughput/total_training_log_Gflops=19.98
    throughput/device/tokens_per_second=68,587
    throughput/device/batches_per_second=0.2616
2025-12-12 12:21:45.774	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=26/20,epoch=0]
    optim/total_grad_norm=0.9915
    train/CrossEntropyLoss=7.863
    train/Perplexity=2,599
    throughput/total_tokens=54,525,952
    throughput/total_training_Gflops=495,275,012
    throughput/total_training_log_Gflops=20.02
    throughput/device/tokens_per_second=68,534
    throughput/device/batches_per_second=0.2614
2025-12-12 12:21:49.582	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=27/20,epoch=0]
    optim/total_grad_norm=0.7333
    train/CrossEntropyLoss=7.874
    train/Perplexity=2,628
    throughput/total_tokens=56,623,104
    throughput/total_training_Gflops=514,324,051
    throughput/total_training_log_Gflops=20.06
    throughput/device/tokens_per_second=68,572
    throughput/device/batches_per_second=0.2616
2025-12-12 12:21:53.391	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=28/20,epoch=0]
    optim/total_grad_norm=0.7755
    train/CrossEntropyLoss=7.834
    train/Perplexity=2,525
    throughput/total_tokens=58,720,256
    throughput/total_training_Gflops=533,373,090
    throughput/total_training_log_Gflops=20.09
    throughput/device/tokens_per_second=68,590
    throughput/device/batches_per_second=0.2617
2025-12-12 12:21:57.204	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=29/20,epoch=0]
    optim/total_grad_norm=0.7829
    train/CrossEntropyLoss=7.871
    train/Perplexity=2,620
    throughput/total_tokens=60,817,408
    throughput/total_training_Gflops=552,422,129
    throughput/total_training_log_Gflops=20.13
    throughput/device/tokens_per_second=68,581
    throughput/device/batches_per_second=0.2616
2025-12-12 12:22:01.082	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=30/20,epoch=0]
    optim/total_grad_norm=0.6734
    train/CrossEntropyLoss=7.813
    train/Perplexity=2,471
    throughput/total_tokens=62,914,560
    throughput/total_training_Gflops=571,471,168
    throughput/total_training_log_Gflops=20.16
    throughput/device/tokens_per_second=68,577
    throughput/device/batches_per_second=0.2616
    System/Peak GPU Memory (MB)=111,477
2025-12-12 12:22:01.088	quick-rain-shines-fin-03:0	train:377	INFO	Training complete
wandb: WARNING The `quiet` argument to `wandb.run.finish()` is deprecated, use `wandb.Settings(quiet=...)` to set this instead.
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading history steps 27-30, summary, console lines 283-312
wandb: 
wandb: Run history:
wandb:                                    System/Peak GPU Memory (MB) ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                                            optim/clipping_rate ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ
wandb:  optim/exp_avg/module.transformer.blocks.0.att_proj.weight.avg ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÅ
wandb:  optim/exp_avg/module.transformer.blocks.0.att_proj.weight.max ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñà
wandb:  optim/exp_avg/module.transformer.blocks.0.att_proj.weight.min ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñà
wandb: optim/exp_avg/module.transformer.blocks.0.att_proj.weight.norm ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:  optim/exp_avg/module.transformer.blocks.0.attn_out.weight.avg ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÅ
wandb:  optim/exp_avg/module.transformer.blocks.0.attn_out.weight.max ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñà
wandb:  optim/exp_avg/module.transformer.blocks.0.attn_out.weight.min ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñÖ‚ñà
wandb: optim/exp_avg/module.transformer.blocks.0.attn_out.weight.norm ‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñà
wandb:                                                         +1,057 ...
wandb: 
wandb: Run summary:
wandb:                                    System/Peak GPU Memory (MB) 111477.22656
wandb:                                            optim/clipping_rate 0
wandb:  optim/exp_avg/module.transformer.blocks.0.att_proj.weight.avg -0.0
wandb:  optim/exp_avg/module.transformer.blocks.0.att_proj.weight.max 0.00019
wandb:  optim/exp_avg/module.transformer.blocks.0.att_proj.weight.min 0.0
wandb: optim/exp_avg/module.transformer.blocks.0.att_proj.weight.norm 0.02793
wandb:  optim/exp_avg/module.transformer.blocks.0.attn_out.weight.avg -0.0
wandb:  optim/exp_avg/module.transformer.blocks.0.attn_out.weight.max 0.00024
wandb:  optim/exp_avg/module.transformer.blocks.0.attn_out.weight.min 0.0
wandb: optim/exp_avg/module.transformer.blocks.0.attn_out.weight.norm 0.03503
wandb:                                                         +1,057 ...
wandb: 
wandb: üöÄ View run wandb-test-run at: https://wandb.ai/antoniolopardo/VecNorm/runs/ujz0ph6t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/antoniolopardo/VecNorm
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./checkpoints/wandb-test/wandb/wandb/run-20251212_121949-ujz0ph6t/logs
[rank0]:[W1212 12:22:05.231855585 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
