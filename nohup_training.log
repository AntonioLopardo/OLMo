W1212 21:37:09.555000 23522 torch/distributed/run.py:803] 
W1212 21:37:09.555000 23522 torch/distributed/run.py:803] *****************************************
W1212 21:37:09.555000 23522 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1212 21:37:09.555000 23522 torch/distributed/run.py:803] *****************************************
2025-12-12 21:37:25.593	round-name-opens-fin-03:0	train:417	INFO	CLI environment prepared
2025-12-12 21:37:26.295	round-name-opens-fin-03:0	train:97	INFO	Configuration:
2025-12-12 21:37:26.295	round-name-opens-fin-03:0	train:98	INFO	TrainConfig(run_name='OLMo-1B-tied_low_seq_len', seed=6198, epoch=None, dry_run=False, model=ModelConfig(d_model=2048, n_heads=16, n_kv_heads=None, clip_qkv=8.0, n_layers=16, mlp_ratio=8, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', block_group_size=1, alibi=False, alibi_bias_max=8.0, rope=True, rope_full_precision=True, rope_theta=10000, flash_attention=True, attention_dropout=0.0, multi_query_attention=False, attention_layer_norm=False, residual_dropout=0.0, embedding_dropout=0.0, embedding_layer_norm=False, layer_norm_type='default', layer_norm_with_affine=False, layer_norm_eps=1e-05, attention_layer_norm_with_affine=False, max_sequence_length=2048, include_bias=False, bias_for_layer_norm=False, scale_logits=False, vocab_size=50280, embedding_size=50304, weight_tying=True, eos_token_id=50279, pad_token_id=1, init_device='cuda', init_fn='normal', init_std=0.02, init_cutoff_factor=None, precision='amp_bf16', scale_emb_init=False, emb_init_std=None, norm_after=False), optimizer=OptimizerConfig(name='adamw', learning_rate=0.0003, weight_decay=0.1, betas=(0.9, 0.95), eps=1e-05, no_decay_norm_and_bias=None, selective_updates=False, decay_norm_and_bias=True, decay_embeddings=True, metrics_log_interval=10, record_update_metrics=False), scheduler=SchedulerConfig(name='cosine_with_warmup', units='steps', t_warmup=2500, t_max=None, alpha_f=0.1, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=None), data=DataConfig(paths=['/home/vec_norm/OLMo/data/dolma_v1_7/dolma_v1_7_30B.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, generate_doc_lengths=False, num_workers=16, drop_last=True, pin_memory=True, prefetch_factor=2, persistent_workers=True, timeout=0, seed=None, instance_filter=None, custom_dataset=None), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=500, tokenizer=TokenizerConfig(identifier='tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json', truncate_direction='right'), save_folder='/home/vec_norm/OLMo/checkpoints/OLMo-1B-untied', remote_save_folder=None, canceled_check_interval=50, save_interval=1000, save_interval_unsharded=500, save_interval_ephemeral=None, save_num_checkpoints_to_keep=3, save_num_unsharded_checkpoints_to_keep=5, save_overwrite=True, force_save_unsharded=False, no_pre_train_checkpoint=False, load_path=None, load_path_sharded_checkpointer=None, try_load_latest_save=False, reset_optimizer_state=False, reset_trainer_state=False, sharded_checkpointer='torch_legacy', new_style_checkpoints=None, max_duration=10000, global_train_batch_size=2048, device_train_batch_size=256, device_train_microbatch_size=32, device_eval_batch_size=32, eval_subset_num_batches=-1, eval_on_load=False, device_train_grad_accum=8, max_grad_norm=1.0, max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='VecNorm', entity='antoniolopardo', group=None, name=None, tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=1), speed_monitor=SpeedMonitorConfig(window_size=20, gpu_flops_available=None), console_log_interval=1, gen1_gc_interval=1, compile=None, distributed_strategy='ddp', fsdp=FSDPConfig(use_orig_params=True, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, wrapping_strategy=None, precision='pure', hybrid_sharding_num_model_replicas=None), ddp=DDPConfig(grad_sync_mode='batch', find_unused_params=False), single=SingleGPUConfig(device='auto'), softmax_auxiliary_loss=False, auxiliary_loss_multiplier=0.0001, time_limit=None, extra_steps_after_cancel=10, early_stopping_factor=None, save_data_indices=True, python_profiling=False, torch_profiling=False, stop_at=None, stop_after=None, activation_checkpointing=None, fused_loss=None, hf_datasets_cache_dir=None, module_outputs_save_steps=None)
2025-12-12 21:37:26.295	round-name-opens-fin-03:0	train:105	INFO	Saving config to /home/vec_norm/OLMo/checkpoints/OLMo-1B-untied/config.yaml
wandb: Currently logged in as: antoniolopardo to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/vec_norm/OLMo/checkpoints/OLMo-1B-untied/wandb/wandb/run-20251212_213726-nxhiichj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-galaxy-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/antoniolopardo/VecNorm
wandb: üöÄ View run at https://wandb.ai/antoniolopardo/VecNorm/runs/nxhiichj
2025-12-12 21:37:29.636	round-name-opens-fin-03:0	olmo.data.iterable_dataset:79	INFO	Saving global data order indices...
2025-12-12 21:37:29.962	round-name-opens-fin-03:0	olmo.data.iterable_dataset:88	INFO	Global data order indices saved to '/home/vec_norm/OLMo/checkpoints/OLMo-1B-untied/train_data/global_indices.npy'
2025-12-12 21:37:30.046	round-name-opens-fin-03:0	train:139	INFO	Building model...
2025-12-12 21:37:30.183	round-name-opens-fin-03:0	olmo.model:1174	INFO	Initializing model parameters...
2025-12-12 21:37:30.184	round-name-opens-fin-03:0	train:141	INFO	Total number of parameters: 1,176,764,416
2025-12-12 21:37:30.184	round-name-opens-fin-03:0	train:142	INFO	Number of non-embedding parameters: 1,073,741,824
2025-12-12 21:37:30.218	round-name-opens-fin-03:0	train:143	INFO	Peak GPU Memory (MB) before ddp: 4710
2025-12-12 21:37:30.218	round-name-opens-fin-03:0	train:155	INFO	Wrapping model with DDP...
2025-12-12 21:37:30.228	round-name-opens-fin-03:0	olmo.model:1174	INFO	Initializing model parameters...
2025-12-12 21:37:30.237	round-name-opens-fin-03:0	train:232	INFO	Peak GPU Memory (MB) after ddp: 9418
2025-12-12 21:37:30.237	round-name-opens-fin-03:0	train:233	INFO	Model:
2025-12-12 21:37:30.237	round-name-opens-fin-03:0	train:234	INFO	DistributedDataParallel(
  (module): OLMo(
    (transformer): ModuleDict(
      (wte): Embedding(50304, 2048)
      (emb_drop): Dropout(p=0.0, inplace=False)
      (ln_f): LayerNorm()
      (blocks): ModuleList(
        (0-15): 16 x OLMoSequentialBlock(
          (dropout): Dropout(p=0.0, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)
          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)
          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
        )
      )
    )
  )
)
2025-12-12 21:37:30.238	round-name-opens-fin-03:0	olmo.optim:944	INFO	Constructing optimizer with 1 param groups
2025-12-12 21:37:30.239	round-name-opens-fin-03:0	train:335	INFO	Saving pre-train checkpoint...
2025-12-12 21:37:30.531	round-name-opens-fin-03:0	olmo.checkpoint:796	INFO	Saving model state...
2025-12-12 21:37:33.877	round-name-opens-fin-03:0	olmo.checkpoint:811	INFO	Saving optim state...
2025-12-12 21:37:33.879	round-name-opens-fin-03:0	olmo.checkpoint:669	INFO	Saving trainer state...
2025-12-12 21:37:33.879	round-name-opens-fin-03:0	olmo.checkpoint:607	INFO	Saving config...
2025-12-12 21:37:34.658	round-name-opens-fin-03:0	train:337	INFO	Checkpoint saved to /home/vec_norm/OLMo/checkpoints/OLMo-1B-untied/step0-unsharded
2025-12-12 21:37:34.659	round-name-opens-fin-03:0	train:340	INFO	Attempting to load pre-train checkpoint...
2025-12-12 21:37:37.475	round-name-opens-fin-03:0	olmo.train:409	INFO	Resetting learning rate...
2025-12-12 21:37:37.475	round-name-opens-fin-03:0	olmo.train:421	INFO	Restoring RNG states...
2025-12-12 21:37:37.739	round-name-opens-fin-03:0	train:344	INFO	Checkpoint successfully loaded
2025-12-12 21:37:37.739	round-name-opens-fin-03:0	train:375	INFO	Starting training...
2025-12-12 21:37:37.883	round-name-opens-fin-03:0	olmo.train:979	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=9,418
2025-12-12 21:37:55.963	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=1/10000,epoch=0]
    optim/total_grad_norm=4.203
    train/CrossEntropyLoss=11.28
    train/Perplexity=78,984
    throughput/total_tokens=4,194,304
    throughput/total_training_Gflops=32,127,729
    throughput/total_training_log_Gflops=17.29
    System/Peak GPU Memory (MB)=183,488
2025-12-12 21:38:02.393	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=2/10000,epoch=0]
    optim/total_grad_norm=4.588
    train/CrossEntropyLoss=10.60
    train/Perplexity=40,261
    throughput/total_tokens=8,388,608
    throughput/total_training_Gflops=64,255,459
    throughput/total_training_log_Gflops=17.98
    throughput/device/tokens_per_second=81,573
    throughput/device/batches_per_second=0.1556
    System/Peak GPU Memory (MB)=192,903
2025-12-12 21:38:08.823	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=3/10000,epoch=0]
    optim/total_grad_norm=7.175
    train/CrossEntropyLoss=10.12
    train/Perplexity=24,757
    throughput/total_tokens=12,582,912
    throughput/total_training_Gflops=96,383,189
    throughput/total_training_log_Gflops=18.38
    throughput/device/tokens_per_second=81,548
    throughput/device/batches_per_second=0.1555
2025-12-12 21:38:15.245	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=4/10000,epoch=0]
    optim/total_grad_norm=7.677
    train/CrossEntropyLoss=9.862
    train/Perplexity=19,178
    throughput/total_tokens=16,777,216
    throughput/total_training_Gflops=128,510,919
    throughput/total_training_log_Gflops=18.67
    throughput/device/tokens_per_second=81,580
    throughput/device/batches_per_second=0.1556
2025-12-12 21:38:21.672	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=5/10000,epoch=0]
    optim/total_grad_norm=4.811
    train/CrossEntropyLoss=9.779
    train/Perplexity=17,653
    throughput/total_tokens=20,971,520
    throughput/total_training_Gflops=160,638,648
    throughput/total_training_log_Gflops=18.89
    throughput/device/tokens_per_second=81,577
    throughput/device/batches_per_second=0.1556
2025-12-12 21:38:28.096	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=6/10000,epoch=0]
    optim/total_grad_norm=3.447
    train/CrossEntropyLoss=9.570
    train/Perplexity=14,325
    throughput/total_tokens=25,165,824
    throughput/total_training_Gflops=192,766,378
    throughput/total_training_log_Gflops=19.08
    throughput/device/tokens_per_second=81,585
    throughput/device/batches_per_second=0.1556
2025-12-12 21:38:34.511	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=7/10000,epoch=0]
    optim/total_grad_norm=3.613
    train/CrossEntropyLoss=9.380
    train/Perplexity=11,852
    throughput/total_tokens=29,360,128
    throughput/total_training_Gflops=224,894,108
    throughput/total_training_log_Gflops=19.23
    throughput/device/tokens_per_second=81,609
    throughput/device/batches_per_second=0.1557
2025-12-12 21:38:40.929	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=8/10000,epoch=0]
    optim/total_grad_norm=3.778
    train/CrossEntropyLoss=9.240
    train/Perplexity=10,302
    throughput/total_tokens=33,554,432
    throughput/total_training_Gflops=257,021,838
    throughput/total_training_log_Gflops=19.36
    throughput/device/tokens_per_second=81,621
    throughput/device/batches_per_second=0.1557
2025-12-12 21:38:47.339	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=9/10000,epoch=0]
    optim/total_grad_norm=6.131
    train/CrossEntropyLoss=9.224
    train/Perplexity=10,134
    throughput/total_tokens=37,748,736
    throughput/total_training_Gflops=289,149,567
    throughput/total_training_log_Gflops=19.48
    throughput/device/tokens_per_second=81,641
    throughput/device/batches_per_second=0.1557
2025-12-12 21:38:53.841	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=10/10000,epoch=0]
    optim/total_grad_norm=3.932
    train/CrossEntropyLoss=9.122
    train/Perplexity=9,158
    throughput/total_tokens=41,943,040
    throughput/total_training_Gflops=321,277,297
    throughput/total_training_log_Gflops=19.59
    throughput/device/tokens_per_second=81,532
    throughput/device/batches_per_second=0.1555
    System/Peak GPU Memory (MB)=192,903
2025-12-12 21:39:00.248	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=11/10000,epoch=0]
    optim/total_grad_norm=2.229
    train/CrossEntropyLoss=9.049
    train/Perplexity=8,512
    throughput/total_tokens=46,137,344
    throughput/total_training_Gflops=353,405,027
    throughput/total_training_log_Gflops=19.68
    throughput/device/tokens_per_second=81,558
    throughput/device/batches_per_second=0.1556
2025-12-12 21:39:06.648	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=12/10000,epoch=0]
    optim/total_grad_norm=2.170
    train/CrossEntropyLoss=8.978
    train/Perplexity=7,926
    throughput/total_tokens=50,331,648
    throughput/total_training_Gflops=385,532,757
    throughput/total_training_log_Gflops=19.77
    throughput/device/tokens_per_second=81,591
    throughput/device/batches_per_second=0.1556
2025-12-12 21:39:13.050	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=13/10000,epoch=0]
    optim/total_grad_norm=2.387
    train/CrossEntropyLoss=8.929
    train/Perplexity=7,550
    throughput/total_tokens=54,525,952
    throughput/total_training_Gflops=417,660,486
    throughput/total_training_log_Gflops=19.85
    throughput/device/tokens_per_second=81,616
    throughput/device/batches_per_second=0.1557
2025-12-12 21:39:19.443	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=14/10000,epoch=0]
    optim/total_grad_norm=2.794
    train/CrossEntropyLoss=8.870
    train/Perplexity=7,116
    throughput/total_tokens=58,720,256
    throughput/total_training_Gflops=449,788,216
    throughput/total_training_log_Gflops=19.92
    throughput/device/tokens_per_second=81,647
    throughput/device/batches_per_second=0.1557
2025-12-12 21:39:25.843	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=15/10000,epoch=0]
    optim/total_grad_norm=1.562
    train/CrossEntropyLoss=8.808
    train/Perplexity=6,686
    throughput/total_tokens=62,914,560
    throughput/total_training_Gflops=481,915,946
    throughput/total_training_log_Gflops=19.99
    throughput/device/tokens_per_second=81,666
    throughput/device/batches_per_second=0.1558
2025-12-12 21:39:32.247	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=16/10000,epoch=0]
    optim/total_grad_norm=2.262
    train/CrossEntropyLoss=8.755
    train/Perplexity=6,341
    throughput/total_tokens=67,108,864
    throughput/total_training_Gflops=514,043,676
    throughput/total_training_log_Gflops=20.06
    throughput/device/tokens_per_second=81,679
    throughput/device/batches_per_second=0.1558
2025-12-12 21:39:38.654	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=17/10000,epoch=0]
    optim/total_grad_norm=2.003
    train/CrossEntropyLoss=8.721
    train/Perplexity=6,129
    throughput/total_tokens=71,303,168
    throughput/total_training_Gflops=546,171,405
    throughput/total_training_log_Gflops=20.12
    throughput/device/tokens_per_second=81,689
    throughput/device/batches_per_second=0.1558
2025-12-12 21:39:45.059	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=18/10000,epoch=0]
    optim/total_grad_norm=1.491
    train/CrossEntropyLoss=8.663
    train/Perplexity=5,786
    throughput/total_tokens=75,497,472
    throughput/total_training_Gflops=578,299,135
    throughput/total_training_log_Gflops=20.18
    throughput/device/tokens_per_second=81,698
    throughput/device/batches_per_second=0.1558
2025-12-12 21:39:51.471	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=19/10000,epoch=0]
    optim/total_grad_norm=1.420
    train/CrossEntropyLoss=8.605
    train/Perplexity=5,459
    throughput/total_tokens=79,691,776
    throughput/total_training_Gflops=610,426,865
    throughput/total_training_log_Gflops=20.23
    throughput/device/tokens_per_second=81,702
    throughput/device/batches_per_second=0.1558
2025-12-12 21:39:57.913	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=20/10000,epoch=0]
    optim/total_grad_norm=1.725
    train/CrossEntropyLoss=8.572
    train/Perplexity=5,279
    throughput/total_tokens=83,886,080
    throughput/total_training_Gflops=642,554,595
    throughput/total_training_log_Gflops=20.28
    throughput/device/tokens_per_second=81,686
    throughput/device/batches_per_second=0.1558
    System/Peak GPU Memory (MB)=192,903
2025-12-12 21:40:04.313	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=21/10000,epoch=0]
    optim/total_grad_norm=1.203
    train/CrossEntropyLoss=8.530
    train/Perplexity=5,065
    throughput/total_tokens=88,080,384
    throughput/total_training_Gflops=674,682,325
    throughput/total_training_log_Gflops=20.33
    throughput/device/tokens_per_second=81,697
    throughput/device/batches_per_second=0.1558
2025-12-12 21:40:10.714	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=22/10000,epoch=0]
    optim/total_grad_norm=1.361
    train/CrossEntropyLoss=8.481
    train/Perplexity=4,823
    throughput/total_tokens=92,274,688
    throughput/total_training_Gflops=706,810,054
    throughput/total_training_log_Gflops=20.38
    throughput/device/tokens_per_second=81,715
    throughput/device/batches_per_second=0.1559
2025-12-12 21:40:17.112	round-name-opens-fin-03:0	olmo.train:979	INFO	[step=23/10000,epoch=0]
    optim/total_grad_norm=1.219
    train/CrossEntropyLoss=8.430
    train/Perplexity=4,583
    throughput/total_tokens=96,468,992
    throughput/total_training_Gflops=738,937,784
    throughput/total_training_log_Gflops=20.42
    throughput/device/tokens_per_second=81,735
    throughput/device/batches_per_second=0.1559
W1212 21:40:20.417000 23522 torch/distributed/elastic/agent/server/api.py:725] Received 15 death signal, shutting down workers
W1212 21:40:20.424000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23679 closing signal SIGTERM
W1212 21:40:20.427000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23680 closing signal SIGTERM
W1212 21:40:20.428000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23681 closing signal SIGTERM
W1212 21:40:20.430000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23682 closing signal SIGTERM
W1212 21:40:20.432000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23683 closing signal SIGTERM
W1212 21:40:20.432000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23684 closing signal SIGTERM
W1212 21:40:20.433000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23685 closing signal SIGTERM
W1212 21:40:20.433000 23522 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 23686 closing signal SIGTERM
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Traceback (most recent call last):
  File "/home/vec_norm/.venv/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 284, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 717, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 881, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/vec_norm/.venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 85, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 23522 got signal: 15
