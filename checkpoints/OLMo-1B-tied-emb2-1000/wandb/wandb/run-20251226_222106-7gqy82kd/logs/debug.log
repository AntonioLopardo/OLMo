2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_setup.py:_flush():80] Current SDK version is 0.23.1
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_setup.py:_flush():80] Configure stats pid to 232313
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_setup.py:_flush():80] Loading settings from /root/.config/wandb/settings
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_setup.py:_flush():80] Loading settings from /home/vec_norm/OLMo/wandb/settings
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_setup.py:_flush():80] Loading settings from environment variables
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_init.py:setup_run_log_directory():714] Logging user logs to /home/vec_norm/OLMo/checkpoints/OLMo-1B-tied-emb2-1000/wandb/wandb/run-20251226_222106-7gqy82kd/logs/debug.log
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_init.py:setup_run_log_directory():715] Logging internal logs to /home/vec_norm/OLMo/checkpoints/OLMo-1B-tied-emb2-1000/wandb/wandb/run-20251226_222106-7gqy82kd/logs/debug-internal.log
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_init.py:init():841] calling init triggers
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_init.py:init():846] wandb.init called with sweep_config: {}
config: {'run_name': 'OLMo-1B-tied-emb2-1000', 'seed': 6198, 'epoch': None, 'dry_run': False, 'model': {'d_model': 2048, 'n_heads': 16, 'n_kv_heads': None, 'clip_qkv': 8.0, 'n_layers': 16, 'mlp_ratio': 8, 'mlp_hidden_size': None, 'activation_type': 'swiglu', 'block_type': 'sequential', 'block_group_size': 1, 'alibi': False, 'alibi_bias_max': 8.0, 'rope': True, 'rope_full_precision': True, 'rope_theta': 10000, 'flash_attention': True, 'attention_dropout': 0.0, 'multi_query_attention': False, 'attention_layer_norm': False, 'residual_dropout': 0.0, 'embedding_dropout': 0.0, 'embedding_layer_norm': False, 'layer_norm_type': 'default', 'layer_norm_with_affine': False, 'layer_norm_eps': 1e-05, 'attention_layer_norm_with_affine': False, 'max_sequence_length': 4096, 'include_bias': False, 'bias_for_layer_norm': False, 'scale_logits': False, 'vocab_size': 50280, 'embedding_size': 50304, 'weight_tying': True, 'track_embedding_gradient_provenance': True, 'clip_output_proj_to_embedding_grad_norm': True, 'output_proj_clip_window_size': 5, 'output_proj_clip_scale_factor': 0.1, 'eos_token_id': 50279, 'pad_token_id': 1, 'init_device': 'cuda', 'init_fn': 'mitchell', 'init_std': 0.02, 'init_cutoff_factor': None, 'precision': 'amp_bf16', 'scale_emb_init': False, 'emb_init_std': None, 'norm_after': False}, 'optimizer': {'name': 'adamw', 'learning_rate': 0.0003, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-05, 'no_decay_norm_and_bias': None, 'selective_updates': False, 'decay_norm_and_bias': True, 'decay_embeddings': True, 'metrics_log_interval': 10, 'record_update_metrics': False}, 'scheduler': {'name': 'cosine_with_warmup', 'units': 'steps', 't_warmup': 100, 't_max': None, 'alpha_f': 0.1, 'grad_clip_warmup_steps': None, 'grad_clip_warmup_factor': None, 'warmup_min_lr': None}, 'data': {'paths': ['/home/vec_norm/OLMo/data/dolma_v1_7/dolma_v1_7_30B.npy'], 'memmap_dtype': 'uint16', 'datasets': None, 'label_mask_paths': None, 'pad_direction': 'right', 'generate_attention_mask': False, 'generate_doc_lengths': False, 'num_workers': 32, 'drop_last': True, 'pin_memory': True, 'prefetch_factor': 4, 'persistent_workers': True, 'timeout': 0, 'seed': None, 'instance_filter': None, 'custom_dataset': None}, 'restore_dataloader': True, 'fast_forward_batches': None, 'evaluators': [], 'eval_interval': 500, 'tokenizer': {'identifier': 'tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json', 'truncate_direction': 'right'}, 'save_folder': '/home/vec_norm/OLMo/checkpoints/OLMo-1B-tied-emb2-1000', 'remote_save_folder': None, 'canceled_check_interval': 50, 'save_interval': 1000, 'save_interval_unsharded': 500, 'save_interval_ephemeral': None, 'save_num_checkpoints_to_keep': 2, 'save_num_unsharded_checkpoints_to_keep': 2, 'save_overwrite': True, 'force_save_unsharded': False, 'no_pre_train_checkpoint': False, 'load_path': None, 'load_path_sharded_checkpointer': None, 'try_load_latest_save': False, 'reset_optimizer_state': False, 'reset_trainer_state': False, 'sharded_checkpointer': 'torch_legacy', 'new_style_checkpoints': None, 'max_duration': 1000, 'global_train_batch_size': 512, 'device_train_batch_size': 64, 'device_train_microbatch_size': 8, 'device_eval_batch_size': 8, 'eval_subset_num_batches': -1, 'eval_on_load': False, 'device_train_grad_accum': 8, 'max_grad_norm': 1.0, 'max_grad_norm_ratio': None, 'precision': 'amp_bf16', 'speed_monitor': {'window_size': 20, 'gpu_flops_available': None}, 'console_log_interval': 10, 'gen1_gc_interval': 1, 'compile': None, 'distributed_strategy': 'ddp', 'fsdp': {'use_orig_params': True, 'sharding_strategy': <ShardingStrategy.FULL_SHARD: 1>, 'wrapping_strategy': None, 'precision': 'pure', 'hybrid_sharding_num_model_replicas': None}, 'ddp': {'grad_sync_mode': 'batch', 'find_unused_params': False}, 'single': {'device': 'auto'}, 'softmax_auxiliary_loss': False, 'auxiliary_loss_multiplier': 0.0001, 'time_limit': None, 'extra_steps_after_cancel': 10, 'early_stopping_factor': None, 'save_data_indices': True, 'python_profiling': False, 'torch_profiling': False, 'stop_at': None, 'stop_after': None, 'activation_checkpointing': None, 'fused_loss': None, 'hf_datasets_cache_dir': None, 'module_outputs_save_steps': None, '_wandb': {}}
2025-12-26 22:21:06,267 INFO    MainThread:232313 [wandb_init.py:init():889] starting backend
2025-12-26 22:21:06,493 INFO    MainThread:232313 [wandb_init.py:init():892] sending inform_init request
2025-12-26 22:21:06,495 INFO    MainThread:232313 [wandb_init.py:init():900] backend started and connected
2025-12-26 22:21:06,497 INFO    MainThread:232313 [wandb_init.py:init():970] updated telemetry
2025-12-26 22:21:06,500 INFO    MainThread:232313 [wandb_init.py:init():994] communicating run to backend with 90.0 second timeout
2025-12-26 22:21:07,126 INFO    MainThread:232313 [wandb_init.py:init():1041] starting run threads in backend
2025-12-26 22:21:07,184 INFO    MainThread:232313 [wandb_run.py:_console_start():2521] atexit reg
2025-12-26 22:21:07,184 INFO    MainThread:232313 [wandb_run.py:_redirect():2369] redirect: wrap_raw
2025-12-26 22:21:07,184 INFO    MainThread:232313 [wandb_run.py:_redirect():2438] Wrapping output streams.
2025-12-26 22:21:07,184 INFO    MainThread:232313 [wandb_run.py:_redirect():2461] Redirects installed.
2025-12-26 22:21:07,186 INFO    MainThread:232313 [wandb_init.py:init():1081] run started, returning control to user process
2025-12-26 23:40:56,111 INFO    MainThread:232313 [wandb_run.py:_finish():2287] finishing run antoniolopardo/VecNorm/7gqy82kd
2025-12-26 23:40:56,112 INFO    MainThread:232313 [wandb_run.py:_atexit_cleanup():2486] got exitcode: 0
2025-12-26 23:40:56,112 INFO    MainThread:232313 [wandb_run.py:_restore():2468] restore
2025-12-26 23:40:56,112 INFO    MainThread:232313 [wandb_run.py:_restore():2474] restore done
2025-12-26 23:40:57,875 INFO    MainThread:232313 [wandb_run.py:_footer_sync_info():3862] logging synced files
