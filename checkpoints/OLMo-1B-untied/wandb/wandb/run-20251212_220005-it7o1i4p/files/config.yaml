_wandb:
    value:
        cli_version: 0.23.1
        e:
            2k4wapgwngpyxjm17aluvenddken7qlw:
                args:
                    - configs/custom/todo/OLMo-1B-tied_low_seq_len.yaml
                codePath: scripts/train.py
                codePathLocal: scripts/train.py
                cpu_count: 240
                cpu_count_logical: 240
                cudaVersion: "13.0"
                disk:
                    /:
                        total: "758162690048"
                        used: "307446411264"
                email: antonio.lopardo@outlook.com
                executable: /home/vec_norm/.venv/bin/python
                git:
                    commit: 090253dac6688f2532509daa7aa2eb5fae50e956
                    remote: https://github.com/allenai/OLMo.git
                gpu: NVIDIA B200
                gpu_count: 8
                gpu_nvidia:
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-b04f3984-6805-4078-ff32-8440ee19da8d
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-66e9ad7d-004d-69e2-07c0-237843834889
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-216ee319-b4b9-76e4-9519-cba6f16f8f59
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-b8038117-6205-9989-3d37-eb811e3ad02c
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-2a1dd45e-1e1a-1557-bbe3-83653c628480
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-7df9b6b4-e67a-3371-cee3-105468b9a3c0
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-6de07ccf-904a-6f00-88d2-6359eedc017c
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-79653909-f3d5-43f2-3823-19520fd107c8
                host: clean-soul-smiles-fin-03
                memory:
                    total: "1555216592896"
                os: Linux-6.8.0-87-generic-x86_64-with-glibc2.39
                program: /home/vec_norm/OLMo/scripts/train.py
                python: CPython 3.12.3
                root: /home/vec_norm/OLMo/checkpoints/OLMo-1B-untied/wandb
                startedAt: "2025-12-12T22:00:05.602308Z"
                writerId: 2k4wapgwngpyxjm17aluvenddken7qlw
        m: []
        python_version: 3.12.3
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
            "3":
                - 2
                - 15
                - 16
                - 61
            "4": 3.12.3
            "5": 0.23.1
            "6": 4.57.1
            "10":
                - 19
            "12": 0.23.1
            "13": linux-x86_64
activation_checkpointing:
    value: null
auxiliary_loss_multiplier:
    value: 0.0001
canceled_check_interval:
    value: 50
compile:
    value: null
console_log_interval:
    value: 1
data:
    value:
        custom_dataset: null
        datasets: null
        drop_last: true
        generate_attention_mask: false
        generate_doc_lengths: false
        instance_filter: null
        label_mask_paths: null
        memmap_dtype: uint16
        num_workers: 16
        pad_direction: right
        paths:
            - /home/vec_norm/OLMo/data/dolma_v1_7/dolma_v1_7_30B.npy
        persistent_workers: true
        pin_memory: true
        prefetch_factor: 2
        seed: null
        timeout: 0
ddp:
    value:
        find_unused_params: false
        grad_sync_mode: batch
device_eval_batch_size:
    value: 32
device_train_batch_size:
    value: 1024
device_train_grad_accum:
    value: 32
device_train_microbatch_size:
    value: 32
distributed_strategy:
    value: ddp
dry_run:
    value: false
early_stopping_factor:
    value: null
epoch:
    value: null
eval_interval:
    value: 500
eval_on_load:
    value: false
eval_subset_num_batches:
    value: -1
evaluators:
    value: []
extra_steps_after_cancel:
    value: 10
fast_forward_batches:
    value: null
force_save_unsharded:
    value: false
fsdp:
    value:
        hybrid_sharding_num_model_replicas: null
        precision: pure
        sharding_strategy: FULL_SHARD
        use_orig_params: true
        wrapping_strategy: null
fused_loss:
    value: null
gen1_gc_interval:
    value: 1
global_train_batch_size:
    value: 1024
hf_datasets_cache_dir:
    value: null
load_path:
    value: null
load_path_sharded_checkpointer:
    value: null
max_duration:
    value: 10000
max_grad_norm:
    value: 1
max_grad_norm_ratio:
    value: null
model:
    value:
        activation_type: swiglu
        alibi: false
        alibi_bias_max: 8
        attention_dropout: 0
        attention_layer_norm: false
        attention_layer_norm_with_affine: false
        bias_for_layer_norm: false
        block_group_size: 1
        block_type: sequential
        clip_qkv: 8
        d_model: 2048
        emb_init_std: null
        embedding_dropout: 0
        embedding_layer_norm: false
        embedding_size: 50304
        eos_token_id: 50279
        flash_attention: true
        include_bias: false
        init_cutoff_factor: null
        init_device: cuda
        init_fn: mitchell
        init_std: 0.02
        layer_norm_eps: 1e-05
        layer_norm_type: default
        layer_norm_with_affine: false
        max_sequence_length: 2048
        mlp_hidden_size: null
        mlp_ratio: 8
        multi_query_attention: false
        n_heads: 16
        n_kv_heads: null
        n_layers: 16
        norm_after: false
        pad_token_id: 1
        precision: amp_bf16
        residual_dropout: 0
        rope: true
        rope_full_precision: true
        rope_theta: 10000
        scale_emb_init: false
        scale_logits: false
        vocab_size: 50280
        weight_tying: true
module_outputs_save_steps:
    value: null
new_style_checkpoints:
    value: null
no_pre_train_checkpoint:
    value: false
optimizer:
    value:
        betas:
            - 0.9
            - 0.95
        decay_embeddings: true
        decay_norm_and_bias: true
        eps: 1e-05
        learning_rate: 0.0003
        metrics_log_interval: 10
        name: adamw
        no_decay_norm_and_bias: null
        record_update_metrics: false
        selective_updates: false
        weight_decay: 0.1
precision:
    value: amp_bf16
python_profiling:
    value: false
remote_save_folder:
    value: null
reset_optimizer_state:
    value: false
reset_trainer_state:
    value: false
restore_dataloader:
    value: true
run_name:
    value: OLMo-1B-tied_low_seq_len
save_data_indices:
    value: true
save_folder:
    value: /home/vec_norm/OLMo/checkpoints/OLMo-1B-untied
save_interval:
    value: 1000
save_interval_ephemeral:
    value: null
save_interval_unsharded:
    value: 500
save_num_checkpoints_to_keep:
    value: 3
save_num_unsharded_checkpoints_to_keep:
    value: 5
save_overwrite:
    value: true
scheduler:
    value:
        alpha_f: 0.1
        grad_clip_warmup_factor: null
        grad_clip_warmup_steps: null
        name: cosine_with_warmup
        t_max: null
        t_warmup: 2500
        units: steps
        warmup_min_lr: null
seed:
    value: 6198
sharded_checkpointer:
    value: torch_legacy
single:
    value:
        device: auto
softmax_auxiliary_loss:
    value: false
speed_monitor:
    value:
        gpu_flops_available: null
        window_size: 20
stop_after:
    value: null
stop_at:
    value: null
time_limit:
    value: null
tokenizer:
    value:
        identifier: tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json
        truncate_direction: right
torch_profiling:
    value: false
try_load_latest_save:
    value: false
