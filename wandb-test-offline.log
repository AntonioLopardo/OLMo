W1212 12:15:47.517000 43897 torch/distributed/run.py:803] 
W1212 12:15:47.517000 43897 torch/distributed/run.py:803] *****************************************
W1212 12:15:47.517000 43897 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1212 12:15:47.517000 43897 torch/distributed/run.py:803] *****************************************
2025-12-12 12:16:07.522	quick-rain-shines-fin-03:0	train:417	INFO	CLI environment prepared
2025-12-12 12:16:08.764	quick-rain-shines-fin-03:0	train:97	INFO	Configuration:
2025-12-12 12:16:08.764	quick-rain-shines-fin-03:0	train:98	INFO	TrainConfig(run_name='wandb-test-run', seed=6198, epoch=None, dry_run=False, model=ModelConfig(d_model=2048, n_heads=16, n_kv_heads=None, clip_qkv=8.0, n_layers=16, mlp_ratio=8, mlp_hidden_size=None, activation_type='swiglu', block_type='sequential', block_group_size=1, alibi=False, alibi_bias_max=8.0, rope=True, rope_full_precision=True, rope_theta=10000, flash_attention=True, attention_dropout=0.0, multi_query_attention=False, attention_layer_norm=False, residual_dropout=0.0, embedding_dropout=0.0, embedding_layer_norm=False, layer_norm_type='default', layer_norm_with_affine=False, layer_norm_eps=1e-05, attention_layer_norm_with_affine=False, max_sequence_length=4096, include_bias=False, bias_for_layer_norm=False, scale_logits=False, vocab_size=50280, embedding_size=50304, weight_tying=False, eos_token_id=50279, pad_token_id=1, init_device='cuda', init_fn='mitchell', init_std=0.02, init_cutoff_factor=None, precision='amp_bf16', scale_emb_init=False, emb_init_std=None, norm_after=False), optimizer=OptimizerConfig(name='adamw', learning_rate=0.0003, weight_decay=0.1, betas=(0.9, 0.95), eps=1e-05, no_decay_norm_and_bias=None, selective_updates=False, decay_norm_and_bias=True, decay_embeddings=True, metrics_log_interval=5, record_update_metrics=False), scheduler=SchedulerConfig(name='cosine_with_warmup', units='steps', t_warmup=10, t_max=None, alpha_f=0.1, grad_clip_warmup_steps=None, grad_clip_warmup_factor=None, warmup_min_lr=None), data=DataConfig(paths=['/home/vec_norm/OLMo/data/dolma_v1_7/dolma_v1_7_30B.npy'], memmap_dtype='uint16', datasets=None, label_mask_paths=None, pad_direction='right', generate_attention_mask=False, generate_doc_lengths=False, num_workers=8, drop_last=True, pin_memory=True, prefetch_factor=2, persistent_workers=True, timeout=0, seed=None, instance_filter=None, custom_dataset=None), restore_dataloader=True, fast_forward_batches=None, evaluators=[], eval_interval=1000, tokenizer=TokenizerConfig(identifier='tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json', truncate_direction='right'), save_folder='/home/vec_norm/OLMo/checkpoints/wandb-test', remote_save_folder=None, canceled_check_interval=50, save_interval=1000, save_interval_unsharded=None, save_interval_ephemeral=None, save_num_checkpoints_to_keep=1, save_num_unsharded_checkpoints_to_keep=0, save_overwrite=True, force_save_unsharded=False, no_pre_train_checkpoint=True, load_path=None, load_path_sharded_checkpointer=None, try_load_latest_save=False, reset_optimizer_state=False, reset_trainer_state=False, sharded_checkpointer='torch_legacy', new_style_checkpoints=None, max_duration=20, global_train_batch_size=512, device_train_batch_size=64, device_train_microbatch_size=8, device_eval_batch_size=16, eval_subset_num_batches=-1, eval_on_load=False, device_train_grad_accum=8, max_grad_norm=1.0, max_grad_norm_ratio=None, precision='amp_bf16', wandb=WandbConfig(project='olmo-test', entity='ai2-llm', group='wandb-tests', name='wandb-test-run', tags=['watching'], log_artifacts=False, rank_zero_only=True, log_interval=1), speed_monitor=SpeedMonitorConfig(window_size=5, gpu_flops_available=None), console_log_interval=1, gen1_gc_interval=1, compile=None, distributed_strategy='ddp', fsdp=FSDPConfig(use_orig_params=True, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, wrapping_strategy=None, precision='pure', hybrid_sharding_num_model_replicas=None), ddp=DDPConfig(grad_sync_mode='batch', find_unused_params=False), single=SingleGPUConfig(device='auto'), softmax_auxiliary_loss=False, auxiliary_loss_multiplier=0.0001, time_limit=None, extra_steps_after_cancel=10, early_stopping_factor=None, save_data_indices=True, python_profiling=False, torch_profiling=False, stop_at=None, stop_after=None, activation_checkpointing=None, fused_loss=None, hf_datasets_cache_dir=None, module_outputs_save_steps=None)
2025-12-12 12:16:08.764	quick-rain-shines-fin-03:0	train:105	INFO	Saving config to /home/vec_norm/OLMo/checkpoints/wandb-test/config.yaml
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /home/vec_norm/OLMo/checkpoints/wandb-test/wandb/wandb/offline-run-20251212_121608-8kwg84kn
2025-12-12 12:16:13.859	quick-rain-shines-fin-03:0	olmo.data.iterable_dataset:79	INFO	Saving global data order indices...
2025-12-12 12:16:14.143	quick-rain-shines-fin-03:0	olmo.data.iterable_dataset:88	INFO	Global data order indices saved to '/home/vec_norm/OLMo/checkpoints/wandb-test/train_data/global_indices.npy'
2025-12-12 12:16:14.270	quick-rain-shines-fin-03:0	train:139	INFO	Building model...
2025-12-12 12:16:14.447	quick-rain-shines-fin-03:0	olmo.model:1174	INFO	Initializing model parameters...
2025-12-12 12:16:14.486	quick-rain-shines-fin-03:0	train:141	INFO	Total number of parameters: 1,279,787,008
2025-12-12 12:16:14.486	quick-rain-shines-fin-03:0	train:142	INFO	Number of non-embedding parameters: 1,176,764,416
2025-12-12 12:16:14.518	quick-rain-shines-fin-03:0	train:143	INFO	Peak GPU Memory (MB) before ddp: 5125
2025-12-12 12:16:14.519	quick-rain-shines-fin-03:0	train:155	INFO	Wrapping model with DDP...
2025-12-12 12:16:14.533	quick-rain-shines-fin-03:0	olmo.model:1174	INFO	Initializing model parameters...
2025-12-12 12:16:14.550	quick-rain-shines-fin-03:0	train:232	INFO	Peak GPU Memory (MB) after ddp: 10244
2025-12-12 12:16:14.550	quick-rain-shines-fin-03:0	train:233	INFO	Model:
2025-12-12 12:16:14.550	quick-rain-shines-fin-03:0	train:234	INFO	DistributedDataParallel(
  (module): OLMo(
    (transformer): ModuleDict(
      (wte): Embedding(50304, 2048)
      (emb_drop): Dropout(p=0.0, inplace=False)
      (ln_f): LayerNorm()
      (blocks): ModuleList(
        (0-15): 16 x OLMoSequentialBlock(
          (dropout): Dropout(p=0.0, inplace=False)
          (act): SwiGLU()
          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)
          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)
          (rotary_emb): RotaryEmbedding()
          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)
          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)
          (attn_norm): LayerNorm()
          (ff_norm): LayerNorm()
        )
      )
      (ff_out): Linear(in_features=2048, out_features=50304, bias=False)
    )
  )
)
2025-12-12 12:16:14.552	quick-rain-shines-fin-03:0	olmo.optim:944	INFO	Constructing optimizer with 1 param groups
2025-12-12 12:16:14.553	quick-rain-shines-fin-03:0	train:375	INFO	Starting training...
2025-12-12 12:16:14.554	quick-rain-shines-fin-03:0	olmo.train:979	INFO	Pre-train system metrics
    System/Peak GPU Memory (MB)=10,244
2025-12-12 12:16:30.569	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=1/20,epoch=0]
    optim/total_grad_norm=11.53
    train/CrossEntropyLoss=11.24
    train/Perplexity=76,058
    throughput/total_tokens=2,097,152
    throughput/total_training_Gflops=19,049,038
    throughput/total_training_log_Gflops=16.76
    System/Peak GPU Memory (MB)=101,234
2025-12-12 12:16:34.383	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=2/20,epoch=0]
    optim/total_grad_norm=103.5
    train/CrossEntropyLoss=12.07
    train/Perplexity=174,523
    throughput/total_tokens=4,194,304
    throughput/total_training_Gflops=38,098,077
    throughput/total_training_log_Gflops=17.46
    throughput/device/tokens_per_second=68,792
    throughput/device/batches_per_second=0.2624
    System/Peak GPU Memory (MB)=111,477
2025-12-12 12:16:38.201	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=3/20,epoch=0]
    optim/total_grad_norm=12.92
    train/CrossEntropyLoss=10.91
    train/Perplexity=54,725
    throughput/total_tokens=6,291,456
    throughput/total_training_Gflops=57,147,116
    throughput/total_training_log_Gflops=17.86
    throughput/device/tokens_per_second=68,715
    throughput/device/batches_per_second=0.2621
2025-12-12 12:16:42.013	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=4/20,epoch=0]
    optim/total_grad_norm=22.57
    train/CrossEntropyLoss=11.55
    train/Perplexity=103,871
    throughput/total_tokens=8,388,608
    throughput/total_training_Gflops=76,196,155
    throughput/total_training_log_Gflops=18.15
    throughput/device/tokens_per_second=68,733
    throughput/device/batches_per_second=0.2622
2025-12-12 12:16:45.974	quick-rain-shines-fin-03:0	olmo.train:979	INFO	[step=5/20,epoch=0]
    optim/total_grad_norm=9.563
    train/CrossEntropyLoss=11.31
    train/Perplexity=81,461
    throughput/total_tokens=10,485,760
    throughput/total_training_Gflops=95,245,194
    throughput/total_training_log_Gflops=18.37
    throughput/device/tokens_per_second=68,076
    throughput/device/batches_per_second=0.2597
